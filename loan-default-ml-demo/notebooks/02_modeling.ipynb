{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "619471bb-4101-4921-ac40-1a74c3006446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (413135, 99)\n",
      "Test  shape: (82107, 99)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Path for processed data\n",
    "proc_dir = Path(\"..\") / \"data\" / \"processed\"\n",
    "\n",
    "# Read data into dataframes\n",
    "train_df = pd.read_parquet(proc_dir / \"train.parquet\")\n",
    "test_df  = pd.read_parquet(proc_dir / \"test.parquet\")\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test  shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d2e721-a0f0-4b40-b039-ab29673ab605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric     : ['int_rate', 'inq_last_6mths', 'inq_last_12m', 'mths_since_recent_inq', 'tot_hi_cred_lim', 'fico_range_high', 'total_bc_limit', 'tot_cur_bal', 'installment', 'total_rev_hi_lim', 'max_bal_bc']\n",
      "Categorical : ['grade', 'home_ownership', 'verification_status', 'purpose']\n",
      "Total kept  : 15\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the leak-free feature list built in 01_eda\n",
    "feat_lists   = joblib.load(proc_dir / \"feature_lists_v2.pkl\")\n",
    "num_keep     = feat_lists[\"num\"]\n",
    "cat_keep     = feat_lists[\"cat\"]\n",
    "final_feats  = num_keep + cat_keep\n",
    "\n",
    "print(\"Numeric     :\", num_keep)\n",
    "print(\"Categorical :\", cat_keep)\n",
    "print(\"Total kept  :\", len(final_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3857831-e254-4b42-ac96-fbe6cfe7340c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\models\\\\pipeline_skeleton.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Numeric branch\n",
    "num_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\",  StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical branch\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\",     OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "# Column transformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_keep),\n",
    "    (\"cat\", cat_pipe, cat_keep)\n",
    "])\n",
    "\n",
    "# Full pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\",   LogisticRegression(max_iter=1000,\n",
    "                                        random_state=42,\n",
    "                                        class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "# Save skeleton\n",
    "models_dir = Path(\"..\") / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "joblib.dump(model_pipeline, models_dir / \"pipeline_skeleton.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdedc38b-e42c-4017-94ff-25d6254e5e23",
   "metadata": {},
   "source": [
    "## üìù Defensive Pipeline Skeleton (with Imputation)\n",
    "\n",
    "**Why keep imputers if the parquet files are already clean?**\n",
    "\n",
    "> **Robustness:** Handles NaNs that may appear in new data (dashboard users, ETL drift, reruns). \n",
    "> **Clarity:** Makes preprocessing self-contained; future readers won‚Äôt wonder ‚Äúwhere did NaNs go?‚Äù \n",
    "\n",
    "**Pipeline Overview:**\n",
    "\n",
    "1. **Numeric features**  \n",
    "    ‚Ä¢ Median impute ‚Üí StandardScaler\n",
    "\n",
    "2. **Categorical features**  \n",
    "    ‚Ä¢ Most-frequent impute ‚Üí OneHotEncoder (`ignore` unknown)\n",
    "\n",
    "3. **Classifier**  \n",
    "    ‚Ä¢ `LogisticRegression`<br>‚Ä¢ `class_weight=\"balanced\"` adjusts for 0.05 % defaults<br>‚Ä¢ `max_iter=1000`, `random_state=42` \n",
    "\n",
    "This skeleton is ready for `.fit()`, metric logging, grid search, and later swap-ins (e.g., XGBoost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9abe941d-b69c-440a-b51e-bc0efa871e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc': 0.5771161072123664, 'recall': 0.4864864864864865, 'accuracy': 0.6745100904916755}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, recall_score, accuracy_score\n",
    "\n",
    "target_col = \"target_default\"\n",
    "\n",
    "X_train, y_train = train_df[final_feats], train_df[target_col]\n",
    "X_test,  y_test  = test_df[final_feats],  test_df[target_col]\n",
    "\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred  = model_pipeline.predict(X_test)\n",
    "y_proba = model_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metrics = {\n",
    "    \"roc_auc\":  roc_auc_score(y_test, y_proba),\n",
    "    \"recall\":   recall_score(y_test, y_pred),\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred)\n",
    "}\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e753e3-3eb1-4d51-9e1e-9fcb7c13b548",
   "metadata": {},
   "source": [
    "## üìù Baseline Metrics Review\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| ROC-AUC | `0.577` |\n",
    "| Recall  | `0.486` |\n",
    "| Accuracy | `0.675` |\n",
    "\n",
    "With only origination-time features and balanced class weights, the model captures ~50 % of charge-offs. This is a sensible starting point given the extreme imbalance (0.05 % positives)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
